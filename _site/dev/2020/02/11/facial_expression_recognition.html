<!DOCTYPE html>
<html lang="en-US">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Facial Expression Recognition Challenge using Convolutional Neural Network | Mikemike Zhu.</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Facial Expression Recognition Challenge using Convolutional Neural Network" />
<meta name="author" content="Mikemike Zhu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Designer / Developer" />
<meta property="og:description" content="Designer / Developer" />
<link rel="canonical" href="http://localhost:4000/dev/2020/02/11/facial_expression_recognition.html" />
<meta property="og:url" content="http://localhost:4000/dev/2020/02/11/facial_expression_recognition.html" />
<meta property="og:site_name" content="Mikemike Zhu." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-11T00:00:00+08:00" />
<script type="application/ld+json">
{"datePublished":"2020-02-11T00:00:00+08:00","dateModified":"2020-02-11T00:00:00+08:00","url":"http://localhost:4000/dev/2020/02/11/facial_expression_recognition.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/dev/2020/02/11/facial_expression_recognition.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/img/icon.jpg"},"name":"Mikemike Zhu"},"author":{"@type":"Person","name":"Mikemike Zhu"},"description":"Designer / Developer","@type":"BlogPosting","headline":"Facial Expression Recognition Challenge using Convolutional Neural Network","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=92889640675b13ee140d0a02800a911444fda676">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
</head>

<body>
    <div class="wrapper">
        <header>
            <h1 class="site-title"><a href="http://localhost:4000/">Mikemike Zhu.</a></h1>

            <p>Designer / Developer</p>

            
            <img class="avatar" src="/assets/img/icon.jpg" alt="Logo" />
            

            <ul>
                <li><a href="http://localhost:4000/">Bio</a></li>
                
                <li>
                    <a href="/my_categories/Dev">Dev</a>
                </li>
                
                <li><a href="http://mikemikezhu.me/">Design</a></li>
            </ul>

            
        </header>
        <section>

            <small>11 February 2020</small>
<h1>Facial Expression Recognition Challenge using Convolutional Neural Network</h1>

<p class="view">by Mikemike Zhu</p>


<h2 id="abstract">Abstract</h2>

<p>Recently, I have developed a mobile game, <a href="https://github.com/mikemikezhu/best-actor-ios">Best Actor</a>, which uses computer vision model to recognize facial expressions. Specifically, the mobile game will randomly display a facial expression label (e.g. surprise) to the player. And the player has to mimic the facial expression so as to achieve a high score. Here I have designed a Convolutional Neural Network in order to perform facial expression recognition in the mobile game. This document will discuss the design and implementation details of the computer vision model.</p>

<h2 id="computer-vision">Computer Vision</h2>

<h3 id="download-data">Download Data</h3>

<p>Considering that the mobile game relies on computer vision model to make predictions on human facial expressions, we need to download the dataset to train the computer vision model. Here we use FER2013 dataset in <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">Challenges in Representation Learning: Facial Expression Recognition Challenge</a> in Kaggle. Therefore, let’s configure Kaggle API and download the training dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="c"># Configure kaggle</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'/root/'</span><span class="p">)</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">.</span><span class="n">kaggle</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'/root/.kaggle'</span><span class="p">)</span>
<span class="err">!</span><span class="n">wget</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">check</span><span class="o">-</span><span class="n">certificate</span> <span class="s">'https://docs.google.com/uc?export=download&amp;id=1Y-o0TVcjehM8SZB3Nt8U3xkyeQu-Nse-'</span> <span class="o">-</span><span class="n">O</span> <span class="n">kaggle</span><span class="o">.</span><span class="n">json</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
<span class="err">!</span><span class="n">ls</span> <span class="o">/</span><span class="n">root</span><span class="o">/.</span><span class="n">kaggle</span>

<span class="c"># Set permissions </span>
<span class="err">!</span><span class="n">chmod</span> <span class="mi">600</span> <span class="o">/</span><span class="n">root</span><span class="o">/.</span><span class="n">kaggle</span><span class="o">/</span><span class="n">kaggle</span><span class="o">.</span><span class="n">json</span>

<span class="c"># Create data folder</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'/content/'</span><span class="p">)</span>
<span class="err">!</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="n">data</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="n">data</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'data'</span><span class="p">)</span>
<span class="err">!</span><span class="n">pwd</span>

<span class="c"># Download data</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">kaggle</span>
<span class="err">!</span><span class="n">kaggle</span> <span class="n">competitions</span> <span class="n">download</span> <span class="o">-</span><span class="n">c</span> <span class="n">challenges</span><span class="o">-</span><span class="ow">in</span><span class="o">-</span><span class="n">representation</span><span class="o">-</span><span class="n">learning</span><span class="o">-</span><span class="n">facial</span><span class="o">-</span><span class="n">expression</span><span class="o">-</span><span class="n">recognition</span><span class="o">-</span><span class="n">challenge</span>

<span class="c"># Unzip data</span>
<span class="err">!</span><span class="n">unzip</span> <span class="n">train</span><span class="o">.</span><span class="n">csv</span><span class="o">.</span><span class="nb">zip</span> <span class="n">train</span><span class="o">.</span><span class="n">csv</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kaggle.json
/content/data
Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)
Downloading icml_face_data.csv.zip to /content/data
 68% 66.0M/96.6M [00:00&lt;00:00, 83.5MB/s]
100% 96.6M/96.6M [00:00&lt;00:00, 244MB/s] 
Downloading example_submission.csv to /content/data
  0% 0.00/7.01k [00:00&lt;?, ?B/s]
100% 7.01k/7.01k [00:00&lt;00:00, 6.91MB/s]
Downloading train.csv.zip to /content/data
 84% 65.0M/77.3M [00:01&lt;00:00, 32.5MB/s]
100% 77.3M/77.3M [00:01&lt;00:00, 48.6MB/s]
Downloading test.csv.zip to /content/data
 47% 9.00M/19.3M [00:00&lt;00:00, 75.3MB/s]
100% 19.3M/19.3M [00:00&lt;00:00, 94.6MB/s]
Downloading fer2013.tar.gz to /content/data
 79% 73.0M/92.0M [00:00&lt;00:00, 225MB/s]
100% 92.0M/92.0M [00:00&lt;00:00, 233MB/s]
Archive:  train.csv.zip
  inflating: train.csv               
</code></pre></div></div>

<h3 id="load-data">Load Data</h3>

<p>The image dataset downloaded from Kaggle is in “.csv” file format. Therefore, we need to load the “train.csv” file, and convert it to numpy array. The training images and labels are saved in “x_train” and “y_train” respectively.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="n">train_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">categories_count</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train.csv'</span><span class="p">)</span> <span class="k">as</span> <span class="n">train</span><span class="p">:</span>

    <span class="c"># Read train.csv file</span>
    <span class="n">csv_reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
    <span class="nb">next</span><span class="p">(</span><span class="n">csv_reader</span><span class="p">)</span>  <span class="c"># Skip the header</span>

    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">csv_reader</span><span class="p">:</span>

        <span class="c"># Append image</span>
        <span class="n">pixels_str</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pixels_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pixels_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)]</span>
        <span class="n">pixels_list</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pixels_list</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'uint8'</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">pixels_list</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">))</span>
        <span class="n">train_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="n">label_str</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c"># Calculate categories count</span>
        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">label_str</span> <span class="ow">in</span> <span class="n">categories_count</span><span class="p">:</span>
            <span class="n">count</span> <span class="o">=</span> <span class="n">categories_count</span><span class="p">[</span><span class="n">label_str</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">categories_count</span><span class="p">[</span><span class="n">label_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span>

        <span class="c"># Append label</span>
        <span class="n">label</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">label_str</span><span class="p">)</span>
        <span class="n">train_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="c"># Create numpy array of train images and labels</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_images</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'x_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y_train shape: {0}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_train shape: (28709, 48, 48)
y_train shape: (28709,)
</code></pre></div></div>

<p>The image dataset provided by Kaggle contains 7 different facial expression categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The distogram of the facial expression categories is displayed as follows.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">categories</span> <span class="o">=</span> <span class="p">(</span><span class="s">'Angry'</span><span class="p">,</span> <span class="s">'Disgust'</span><span class="p">,</span> <span class="s">'Fear'</span><span class="p">,</span> <span class="s">'Happy'</span><span class="p">,</span> <span class="s">'Sad'</span><span class="p">,</span> <span class="s">'Surprise'</span><span class="p">,</span> <span class="s">'Neutral'</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">))</span>

<span class="n">counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">)):</span>
    <span class="n">label_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">categories_count</span><span class="p">[</span><span class="n">label_str</span><span class="p">]</span>
    <span class="n">counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">count</span> <span class="o">+</span> <span class="mi">15</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>

<span class="c"># Draw histogram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">categories</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Count'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'FER2013 Dataset Categories'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_11_0.png" alt="png" /></p>

<p>Then, let’s show one of the images in the dataset. Each image is grey-scale and contains 48 x 48 pixels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Label is: '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Label is: 0





&lt;matplotlib.image.AxesImage at 0x7fc0e5a696a0&gt;
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_13_2.png" alt="png" /></p>

<h3 id="preprocess-data">Preprocess Data</h3>

<p>Next, we need to split the dataset into training set and test set. Here, we choose 20% of the dataset as test set, and the rest of the dataset as train set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c"># Split dataset into train set and test set</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(22967, 48, 48, 1)
(5742, 48, 48, 1)
</code></pre></div></div>

<h3 id="train-model">Train Model</h3>

<p>Next, we need to train the model with the image dataset. Here we use Tensorlow as backend to train the model. Therefore, we need to import all the required packages before the training process.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">tensorflow_version</span> <span class="mf">2.</span><span class="n">x</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Tensorflow version: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">MaxPool2D</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ReduceLROnPlateau</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Input</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorFlow 2.x selected.
Tensorflow version: 2.1.0
</code></pre></div></div>

<p>By referring to VGGNet architecture, we have designed the computer vision model with several stacks of layers. The model will have the following components:</p>
<ul>
  <li>Convolutional layers: These layers are the building blocks of our architecture, which learns the image feature by computing the dot product between the weights and the small region on the image. Similar to VGGNet architecture, all the convolutional layers are designed with 3 x 3 kernal size, and several filters.</li>
  <li>Activation functions: The activation functions are those functions which are applied to the outputs of the layers in the network. Specifically, we use ReLU (Rectified Linear Unit) activation function to increase the non-linearity of the network. Besides, a Softmax function will be used to compute the probability of each category.</li>
  <li>Pooling layers: These layers will down-sample the image to reduce the spatial data and extract features. In our model, we will use Max Pooling with A 3 x 3 pooling size and a 2 x 2 stride.</li>
  <li>Dense layers: The dense layers are stacked as the fully connected layers of the network, which take in the feature data from the previous convolutional layers and perform decision making.</li>
  <li>Dropout layers: The dropout layers are used to prevent over-fitting when training the model.</li>
  <li>Batch normalization: This technique can be used to speed up learning by normalizing the output of the previous activation layer.</li>
</ul>

<p>The diagram of the model is displayed as follows.</p>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/cnn.png" alt="cnn" /></p>

<p>Our model contains 5 stacks of layers. In each of the first 4 stacks of layers, there are 2 convolutional layer followed by 1 pooling layer. Besides, we use batch normalization to speed up training and dropout to prevent over-fitting. Then we have one stack of 3 fully-connected layers, followed by a Softmax activation function, which generates the probability of the facial expression categories. Finally, we compile our model using Adam optimizer with a certain learning rate. Considering that we are dealing with classification issue, we will use <code class="highlighter-rouge">sparse_categorical_crossentropy</code> as the loss function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cnn_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># 1st convolution layer</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c"># 2nd convolution layer</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c"># 3rd convolution layer</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c"># 4th convolution layer</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="c"># Fully connected layer</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>

<span class="n">cnn_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="c"># Compile the model</span>
<span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">adam</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s">'sparse_categorical_crossentropy'</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

<span class="c"># Summary the model</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 46, 46, 64)        640       
_________________________________________________________________
batch_normalization (BatchNo (None, 46, 46, 64)        256       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 46, 46, 64)        36928     
_________________________________________________________________
batch_normalization_1 (Batch (None, 46, 46, 64)        256       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         
_________________________________________________________________
dropout (Dropout)            (None, 22, 22, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 22, 22, 128)       73856     
_________________________________________________________________
batch_normalization_2 (Batch (None, 22, 22, 128)       512       
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 22, 22, 128)       147584    
_________________________________________________________________
batch_normalization_3 (Batch (None, 22, 22, 128)       512       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 10, 10, 128)       0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 10, 256)       295168    
_________________________________________________________________
batch_normalization_4 (Batch (None, 10, 10, 256)       1024      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 10, 10, 256)       590080    
_________________________________________________________________
batch_normalization_5 (Batch (None, 10, 10, 256)       1024      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4, 4, 256)         0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 4, 4, 512)         1180160   
_________________________________________________________________
batch_normalization_6 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_7 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 512)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1, 1, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
dense (Dense)                (None, 512)               262656    
_________________________________________________________________
dropout_4 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
dropout_5 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                16448     
_________________________________________________________________
dropout_6 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 7)                 455       
=================================================================
Total params: 5,102,791
Trainable params: 5,098,951
Non-trainable params: 3,840
_________________________________________________________________
</code></pre></div></div>

<p>Next, we will train our model. Here we use Early Stopping strategy, which will stop the training process when there is no improvement in the validation accuracy. Besides, we will also reduce the learning rate by a specific factor if there is a plateau is detected in the validation loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Callbacks</span>
<span class="n">early_stopping</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'val_accuracy'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">reduce_learning_rate</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'val_loss'</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c"># Train the model</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
                        <span class="n">y_train</span><span class="p">,</span>
                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">,</span> <span class="n">reduce_learning_rate</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 22967 samples, validate on 5742 samples
Epoch 1/100
22967/22967 [==============================] - 26s 1ms/sample - loss: 1.8757 - accuracy: 0.2304 - val_loss: 1.8284 - val_accuracy: 0.2529
Epoch 2/100
22967/22967 [==============================] - 16s 678us/sample - loss: 1.7341 - accuracy: 0.2934 - val_loss: 1.6207 - val_accuracy: 0.3715
Epoch 3/100
22967/22967 [==============================] - 16s 681us/sample - loss: 1.5329 - accuracy: 0.3879 - val_loss: 2.3653 - val_accuracy: 0.2847
Epoch 4/100
22967/22967 [==============================] - 16s 682us/sample - loss: 1.4278 - accuracy: 0.4273 - val_loss: 1.4887 - val_accuracy: 0.4136
Epoch 5/100
22967/22967 [==============================] - 16s 685us/sample - loss: 1.3583 - accuracy: 0.4694 - val_loss: 1.3731 - val_accuracy: 0.4929
Epoch 6/100
22967/22967 [==============================] - 16s 687us/sample - loss: 1.2918 - accuracy: 0.5003 - val_loss: 1.4911 - val_accuracy: 0.4260
Epoch 7/100
22967/22967 [==============================] - 16s 692us/sample - loss: 1.2477 - accuracy: 0.5258 - val_loss: 1.2053 - val_accuracy: 0.5381
Epoch 8/100
22967/22967 [==============================] - 16s 692us/sample - loss: 1.2135 - accuracy: 0.5412 - val_loss: 1.2070 - val_accuracy: 0.5347
Epoch 9/100
22967/22967 [==============================] - 16s 688us/sample - loss: 1.1723 - accuracy: 0.5600 - val_loss: 1.2278 - val_accuracy: 0.5369
Epoch 10/100
22967/22967 [==============================] - 16s 686us/sample - loss: 1.1405 - accuracy: 0.5765 - val_loss: 1.1934 - val_accuracy: 0.5589
Epoch 11/100
22967/22967 [==============================] - 16s 687us/sample - loss: 1.1002 - accuracy: 0.5925 - val_loss: 1.1409 - val_accuracy: 0.5778
Epoch 12/100
22967/22967 [==============================] - 16s 685us/sample - loss: 1.0595 - accuracy: 0.6106 - val_loss: 1.1360 - val_accuracy: 0.5655
Epoch 13/100
22967/22967 [==============================] - 16s 689us/sample - loss: 1.0324 - accuracy: 0.6188 - val_loss: 1.1228 - val_accuracy: 0.5881
Epoch 14/100
22967/22967 [==============================] - 16s 687us/sample - loss: 0.9968 - accuracy: 0.6373 - val_loss: 1.1030 - val_accuracy: 0.5876
Epoch 15/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.9516 - accuracy: 0.6552 - val_loss: 1.1632 - val_accuracy: 0.6075
Epoch 16/100
22967/22967 [==============================] - 16s 688us/sample - loss: 0.9075 - accuracy: 0.6700 - val_loss: 1.0542 - val_accuracy: 0.6146
Epoch 17/100
22967/22967 [==============================] - 16s 691us/sample - loss: 0.8639 - accuracy: 0.6882 - val_loss: 1.1217 - val_accuracy: 0.6099
Epoch 18/100
22967/22967 [==============================] - 16s 688us/sample - loss: 0.8284 - accuracy: 0.7018 - val_loss: 1.0902 - val_accuracy: 0.6160
Epoch 19/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.7877 - accuracy: 0.7165 - val_loss: 1.1438 - val_accuracy: 0.6099
Epoch 20/100
22967/22967 [==============================] - 16s 687us/sample - loss: 0.7511 - accuracy: 0.7337 - val_loss: 1.1213 - val_accuracy: 0.6256
Epoch 21/100
22967/22967 [==============================] - 16s 687us/sample - loss: 0.7104 - accuracy: 0.7469 - val_loss: 1.1857 - val_accuracy: 0.6331
Epoch 22/100
22967/22967 [==============================] - 16s 683us/sample - loss: 0.6539 - accuracy: 0.7672 - val_loss: 1.1355 - val_accuracy: 0.6440
Epoch 23/100
22967/22967 [==============================] - 16s 686us/sample - loss: 0.6118 - accuracy: 0.7833 - val_loss: 1.1909 - val_accuracy: 0.6404
Epoch 24/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.5708 - accuracy: 0.7972 - val_loss: 1.1670 - val_accuracy: 0.6444
Epoch 25/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.5430 - accuracy: 0.8112 - val_loss: 1.2351 - val_accuracy: 0.6385
Epoch 26/100
22967/22967 [==============================] - 16s 688us/sample - loss: 0.4982 - accuracy: 0.8281 - val_loss: 1.2673 - val_accuracy: 0.6452
Epoch 27/100
22967/22967 [==============================] - 16s 683us/sample - loss: 0.4561 - accuracy: 0.8412 - val_loss: 1.4775 - val_accuracy: 0.6301
Epoch 28/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.4233 - accuracy: 0.8544 - val_loss: 1.2860 - val_accuracy: 0.6541
Epoch 29/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.3880 - accuracy: 0.8667 - val_loss: 1.3489 - val_accuracy: 0.6423
Epoch 30/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.3702 - accuracy: 0.8762 - val_loss: 1.3640 - val_accuracy: 0.6627
Epoch 31/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.3567 - accuracy: 0.8763 - val_loss: 1.3474 - val_accuracy: 0.6630
Epoch 32/100
22967/22967 [==============================] - 16s 689us/sample - loss: 0.3131 - accuracy: 0.8951 - val_loss: 1.6062 - val_accuracy: 0.6468
Epoch 33/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.2903 - accuracy: 0.9026 - val_loss: 1.4864 - val_accuracy: 0.6484
Epoch 34/100
22967/22967 [==============================] - 16s 686us/sample - loss: 0.2759 - accuracy: 0.9081 - val_loss: 1.4937 - val_accuracy: 0.6344
Epoch 35/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.2593 - accuracy: 0.9142 - val_loss: 1.6131 - val_accuracy: 0.6491
Epoch 36/100
22967/22967 [==============================] - 16s 686us/sample - loss: 0.2403 - accuracy: 0.9203 - val_loss: 1.6904 - val_accuracy: 0.6479
Epoch 37/100
22967/22967 [==============================] - 16s 684us/sample - loss: 0.2291 - accuracy: 0.9262 - val_loss: 1.6712 - val_accuracy: 0.6503
Epoch 38/100
22967/22967 [==============================] - 16s 684us/sample - loss: 0.2120 - accuracy: 0.9293 - val_loss: 1.6716 - val_accuracy: 0.6466
Epoch 39/100
22967/22967 [==============================] - 16s 686us/sample - loss: 0.1862 - accuracy: 0.9371 - val_loss: 1.9402 - val_accuracy: 0.6531
Epoch 40/100
22967/22967 [==============================] - 16s 685us/sample - loss: 0.1930 - accuracy: 0.9347 - val_loss: 1.7538 - val_accuracy: 0.6555
Epoch 41/100
22967/22967 [==============================] - 16s 686us/sample - loss: 0.1779 - accuracy: 0.9419 - val_loss: 1.9947 - val_accuracy: 0.6383
</code></pre></div></div>

<p>After the training process is completed, let’s display the accuracy diagram of the training accuracy and validation accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Show accuracy diagram</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Model Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'val_accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Accuracy'</span><span class="p">,</span> <span class="s">'Validation Accuracy'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_27_0.png" alt="png" /></p>

<h3 id="improve-model">Improve Model</h3>

<p>We will try to improve our model to increase the validation accuracy. Here we have used the following techniques in our model improvement:</p>
<ul>
  <li>Generate hard data</li>
  <li>Data augmentation</li>
</ul>

<p>Before we improve our model, let’s evaluate our model by using test set. We can see the score before improvement is around 64%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Evaluate the model before improvement</span>
<span class="n">_</span><span class="p">,</span> <span class="n">score_before_improvement</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Score before improvement: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score_before_improvement</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>5742/5742 [==============================] - 2s 337us/sample - loss: 1.9947 - accuracy: 0.6383
Score before improvement: 0.6382793188095093
</code></pre></div></div>

<p>Next, we will improve our model by generating hard data. Specifically, instead of training the model over and over again, we will select the images which are incorrectly labelled by the model, and train the model on these specific images. Therefore, let’s use our model to make predictions first, and put the incorrect ones into the array of hard data for further training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate hard data</span>
<span class="n">hard_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">hard_labels</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># Make predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
    <span class="n">y_predict</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">y_real</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">y_predict</span> <span class="o">!=</span> <span class="n">y_real</span><span class="p">:</span>
        <span class="c"># If predict incorrectly, append to array</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">hard_image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">hard_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="n">hard_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_real</span><span class="p">)</span>

<span class="n">x_hard</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hard_images</span><span class="p">)</span>
<span class="n">y_hard</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hard_labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">x_hard</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_hard</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(4775, 48, 48, 1)
(4775,)
</code></pre></div></div>

<p>Next, we will train our model on these specific images which are previously incorrectly labelled by our model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train the model on hard data</span>
<span class="n">x_hard_train</span><span class="p">,</span> <span class="n">x_hard_test</span><span class="p">,</span> <span class="n">y_hard_train</span><span class="p">,</span> <span class="n">y_hard_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x_hard</span><span class="p">,</span> <span class="n">y_hard</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_hard_train</span><span class="p">,</span>
                        <span class="n">y_hard_train</span><span class="p">,</span>
                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_hard_test</span><span class="p">,</span> <span class="n">y_hard_test</span><span class="p">),</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">,</span> <span class="n">reduce_learning_rate</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 3820 samples, validate on 955 samples
Epoch 1/100
3820/3820 [==============================] - 4s 1ms/sample - loss: 0.1810 - accuracy: 0.9406 - val_loss: 0.0398 - val_accuracy: 0.9874
Epoch 2/100
3820/3820 [==============================] - 3s 685us/sample - loss: 0.1333 - accuracy: 0.9542 - val_loss: 0.0413 - val_accuracy: 0.9832
Epoch 3/100
3820/3820 [==============================] - 3s 686us/sample - loss: 0.0935 - accuracy: 0.9691 - val_loss: 0.0455 - val_accuracy: 0.9864
Epoch 4/100
3820/3820 [==============================] - 3s 693us/sample - loss: 0.1113 - accuracy: 0.9636 - val_loss: 0.1021 - val_accuracy: 0.9686
Epoch 5/100
3820/3820 [==============================] - 3s 687us/sample - loss: 0.0914 - accuracy: 0.9741 - val_loss: 0.0544 - val_accuracy: 0.9801
Epoch 6/100
3820/3820 [==============================] - 3s 689us/sample - loss: 0.0755 - accuracy: 0.9775 - val_loss: 0.0646 - val_accuracy: 0.9791
Epoch 7/100
3820/3820 [==============================] - 3s 698us/sample - loss: 0.0540 - accuracy: 0.9830 - val_loss: 0.0576 - val_accuracy: 0.9853
Epoch 8/100
3820/3820 [==============================] - 3s 684us/sample - loss: 0.0535 - accuracy: 0.9853 - val_loss: 0.0505 - val_accuracy: 0.9843
Epoch 9/100
3820/3820 [==============================] - 3s 687us/sample - loss: 0.0655 - accuracy: 0.9832 - val_loss: 0.0521 - val_accuracy: 0.9791
Epoch 10/100
3820/3820 [==============================] - 3s 684us/sample - loss: 0.0469 - accuracy: 0.9872 - val_loss: 0.0713 - val_accuracy: 0.9749
Epoch 11/100
3820/3820 [==============================] - 3s 688us/sample - loss: 0.0427 - accuracy: 0.9861 - val_loss: 0.0847 - val_accuracy: 0.9738
</code></pre></div></div>

<p>Of course, this might cause the over-fitting issue on these incorrectly labelled images. Therefore, we will train the model again to balance out. Besides, we will also perform data augmentation to diversify our training dataset by rotating, shifting, zooming or flipping the images, which will also improve our model to overcome the over-fitting issue and learn the generic features of each image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Perform data augmentation</span>
<span class="n">data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">featurewise_center</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                    <span class="n">featurewise_std_normalization</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                    <span class="n">zoom_range</span><span class="o">=.</span><span class="mi">1</span><span class="p">,</span>
                                    <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">flow</span> <span class="o">=</span> <span class="n">data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> 
                           <span class="n">y_train</span><span class="p">,</span> 
                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="c"># Train the model again to balance out</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">flow</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">,</span> <span class="n">reduce_learning_rate</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
Train for 359 steps, validate on 5742 samples
Epoch 1/100
359/359 [==============================] - 17s 46ms/step - loss: 1.1197 - accuracy: 0.6247 - val_loss: 0.9586 - val_accuracy: 0.6606
Epoch 2/100
359/359 [==============================] - 16s 44ms/step - loss: 1.0057 - accuracy: 0.6500 - val_loss: 0.9215 - val_accuracy: 0.6658
Epoch 3/100
359/359 [==============================] - 16s 45ms/step - loss: 0.9738 - accuracy: 0.6624 - val_loss: 0.9442 - val_accuracy: 0.6503
Epoch 4/100
359/359 [==============================] - 16s 45ms/step - loss: 0.9450 - accuracy: 0.6672 - val_loss: 0.9149 - val_accuracy: 0.6649
Epoch 5/100
359/359 [==============================] - 16s 44ms/step - loss: 0.9308 - accuracy: 0.6700 - val_loss: 0.9238 - val_accuracy: 0.6644
Epoch 6/100
359/359 [==============================] - 16s 44ms/step - loss: 0.9182 - accuracy: 0.6749 - val_loss: 0.9165 - val_accuracy: 0.6703
Epoch 7/100
359/359 [==============================] - 16s 45ms/step - loss: 0.9090 - accuracy: 0.6790 - val_loss: 0.9085 - val_accuracy: 0.6686
Epoch 8/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8925 - accuracy: 0.6882 - val_loss: 0.9234 - val_accuracy: 0.6665
Epoch 9/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8741 - accuracy: 0.6917 - val_loss: 0.9059 - val_accuracy: 0.6769
Epoch 10/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8735 - accuracy: 0.6913 - val_loss: 0.9145 - val_accuracy: 0.6715
Epoch 11/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8649 - accuracy: 0.6968 - val_loss: 0.9292 - val_accuracy: 0.6688
Epoch 12/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8503 - accuracy: 0.6975 - val_loss: 0.9193 - val_accuracy: 0.6670
Epoch 13/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8471 - accuracy: 0.7030 - val_loss: 0.9295 - val_accuracy: 0.6761
Epoch 14/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8217 - accuracy: 0.7105 - val_loss: 0.9447 - val_accuracy: 0.6654
Epoch 15/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8199 - accuracy: 0.7114 - val_loss: 0.9077 - val_accuracy: 0.6790
Epoch 16/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8071 - accuracy: 0.7120 - val_loss: 0.9207 - val_accuracy: 0.6731
Epoch 17/100
359/359 [==============================] - 16s 45ms/step - loss: 0.8013 - accuracy: 0.7132 - val_loss: 0.9483 - val_accuracy: 0.6604
Epoch 18/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7953 - accuracy: 0.7178 - val_loss: 0.9149 - val_accuracy: 0.6769
Epoch 19/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7841 - accuracy: 0.7217 - val_loss: 0.9085 - val_accuracy: 0.6721
Epoch 20/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7696 - accuracy: 0.7271 - val_loss: 0.9132 - val_accuracy: 0.6754
Epoch 21/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7627 - accuracy: 0.7309 - val_loss: 0.9231 - val_accuracy: 0.6705
Epoch 22/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7589 - accuracy: 0.7310 - val_loss: 0.9336 - val_accuracy: 0.6801
Epoch 23/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7578 - accuracy: 0.7276 - val_loss: 0.9242 - val_accuracy: 0.6726
Epoch 24/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7332 - accuracy: 0.7361 - val_loss: 0.9237 - val_accuracy: 0.6848
Epoch 25/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7313 - accuracy: 0.7412 - val_loss: 0.9434 - val_accuracy: 0.6705
Epoch 26/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7280 - accuracy: 0.7431 - val_loss: 0.9376 - val_accuracy: 0.6722
Epoch 27/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7067 - accuracy: 0.7521 - val_loss: 0.9273 - val_accuracy: 0.6796
Epoch 28/100
359/359 [==============================] - 16s 45ms/step - loss: 0.7059 - accuracy: 0.7536 - val_loss: 0.9347 - val_accuracy: 0.6787
Epoch 29/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6980 - accuracy: 0.7553 - val_loss: 0.9211 - val_accuracy: 0.6822
Epoch 30/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6917 - accuracy: 0.7563 - val_loss: 0.9556 - val_accuracy: 0.6738
Epoch 31/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6945 - accuracy: 0.7567 - val_loss: 0.9342 - val_accuracy: 0.6750
Epoch 32/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6782 - accuracy: 0.7609 - val_loss: 0.9296 - val_accuracy: 0.6846
Epoch 33/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6734 - accuracy: 0.7624 - val_loss: 0.9382 - val_accuracy: 0.6827
Epoch 34/100
359/359 [==============================] - 16s 45ms/step - loss: 0.6682 - accuracy: 0.7625 - val_loss: 0.9501 - val_accuracy: 0.6782
</code></pre></div></div>

<p>Finally, let’s evaluate our model again after improvement. Previously, we have the validation accuracy at around 64%. Nevertheless, the validation accuracy has increased to around <strong>68%</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Evaluate the model after improvement</span>
<span class="n">_</span><span class="p">,</span> <span class="n">score_after_improvement</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Score after improvement: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score_after_improvement</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>5742/5742 [==============================] - 2s 261us/sample - loss: 0.9501 - accuracy: 0.6782
Score after improvement: 0.6781609058380127
</code></pre></div></div>

<p>If we compare with the Kaggle competition, our score is around 68% and can be ranked within the top 5.</p>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/kaggle.png" alt="kaggle" /></p>

<h3 id="evaluate-model">Evaluate Model</h3>

<p>Next, we will make predictions and create the confusion matrix. Since FER2013 dataset does NOT provide too many images labelled with “Disgust”, we can tell from the confusion matrix that, the model might not be able to classify the images labelled with “Disgust” very well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">mlxtend</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="c"># Make predictions</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">y_predict</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Create confusion matrix</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">)</span>

<span class="c"># Display confusion matrix</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Angry'</span><span class="p">,</span> <span class="s">'Disgust'</span><span class="p">,</span> <span class="s">'Fear'</span><span class="p">,</span> <span class="s">'Happy'</span><span class="p">,</span> <span class="s">'Sad'</span><span class="p">,</span> <span class="s">'Surprise'</span><span class="p">,</span> <span class="s">'Neutral'</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_mat</span><span class="o">=</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Confusion Matrix'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_44_0.png" alt="png" /></p>

<p>Then, let’s try some other human facial expression images found online.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Download into data folder</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'/content/data'</span><span class="p">)</span>

<span class="err">!</span><span class="n">wget</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">check</span><span class="o">-</span><span class="n">certificate</span> <span class="s">'https://docs.google.com/uc?export=download&amp;id=1ommGLsYSnmX8846iyQigKQfIfvpQzsF0'</span> <span class="o">-</span><span class="n">O</span> <span class="n">happy</span><span class="o">.</span><span class="n">jpg</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
<span class="err">!</span><span class="n">wget</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">check</span><span class="o">-</span><span class="n">certificate</span> <span class="s">'https://docs.google.com/uc?export=download&amp;id=1I1IjDm9Az4pkDqOz3zcQAguEJ1OsWagt'</span> <span class="o">-</span><span class="n">O</span> <span class="n">sad</span><span class="o">.</span><span class="n">jpg</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
<span class="err">!</span><span class="n">wget</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">check</span><span class="o">-</span><span class="n">certificate</span> <span class="s">'https://docs.google.com/uc?export=download&amp;id=1ZLglz_y0QklbYTcwJW1wH0YnAwBn6-1I'</span> <span class="o">-</span><span class="n">O</span> <span class="n">surprise</span><span class="o">.</span><span class="n">jpg</span> <span class="o">&gt;</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span>

<span class="k">def</span> <span class="nf">predict_facial_expression</span><span class="p">(</span><span class="n">image_path</span><span class="p">):</span>
    <span class="s">"""Predict facial expression for image"""</span>
    <span class="c"># Open image</span>
    <span class="n">rgb_image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">image_path</span><span class="p">,</span>
                            <span class="n">color_mode</span><span class="o">=</span><span class="s">'grayscale'</span><span class="p">,</span>
                            <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">))</span>
    <span class="c"># Convert to array</span>
    <span class="n">rgb_array</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">)</span>
    <span class="n">rgb_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">rgb_array</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c"># Show image</span>
    <span class="n">rgb_display</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rgb_array</span><span class="p">,</span> <span class="s">'float32'</span><span class="p">)</span>
    <span class="n">rgb_display</span> <span class="o">=</span> <span class="n">rgb_display</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">]);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">rgb_display</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c"># Make predictions</span>
    <span class="n">rgb_array</span> <span class="o">=</span> <span class="n">rgb_array</span> <span class="o">/</span> <span class="mf">255.0</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">cnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rgb_array</span><span class="p">)</span>

    <span class="c"># Draw the histogram</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s">'center'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">categories</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Percentage'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Facial Expression Prediction'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Happy</span>
<span class="n">predict_facial_expression</span><span class="p">(</span><span class="s">'happy.jpg'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_48_0.png" alt="png" /></p>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_48_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Sad</span>
<span class="n">predict_facial_expression</span><span class="p">(</span><span class="s">'sad.jpg'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_49_0.png" alt="png" /></p>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_49_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Surprise</span>
<span class="n">predict_facial_expression</span><span class="p">(</span><span class="s">'surprise.jpg'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_50_0.png" alt="png" /></p>

<p><img src="/assets/img/2020-02-11-facial_expression_recognition/facial_expression_recognition_50_1.png" alt="png" /></p>

<h3 id="export-model">Export Model</h3>

<p>Last but not least, let’s export the model, and save our TensorFlow and TensorFlow Lite model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create build folder</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'/content/'</span><span class="p">)</span>
<span class="err">!</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="n">build</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="n">build</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s">'build'</span><span class="p">)</span>
<span class="err">!</span><span class="n">pwd</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/content/build
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Save tensorflow model</span>
<span class="n">cnn_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'FER2013.h5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Save tensorflow lite model</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_keras_model</span><span class="p">(</span><span class="n">cnn_model</span><span class="p">)</span>
<span class="n">tflite_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>
<span class="nb">open</span><span class="p">(</span><span class="s">"FER2013.tflite"</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">tflite_model</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>20406252
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>To put it in a nutshell, Best Actor is a mobile game which integrates the computer vision technology using Convolutional Neural Network. Please find my <a href="https://github.com/mikemikezhu/facial-expression-recognition">GitHub Project</a> to download the project.</p>



  <small>tags: <em>machine_learning</em> - <em>cnn</em></small>



        </section>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
</body>

</html>